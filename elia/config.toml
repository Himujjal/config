# the ID or name of the model that is selected by default on launch
default_model = "meta-llama/llama-4-maverick-17b-128e-instruct"
# the system prompt on launch
system_prompt = "You are a helpful assistant who talks like a pirate."

# choose from "nebula", "cobalt", "twilight", "hacker", "alpine", "galaxy", "nautilus", "monokai", "textual"
theme = "alpine"

# change the syntax highlighting theme of code in messages
# choose from https://pygments.org/styles/
# defaults to "monokai"
message_code_theme = "alpine"

# # example of adding local llama3 support
# # only the `name` field is required here.
# [[models]]
# name = "ollama/llama3"

# # example of a model running on a local server, e.g. LocalAI
# [[models]]
# name = "openai/some-model"
# api_base = "http://localhost:8080/v1"
# api_key = "api-key-if-required"

# example of add a groq model, showing some other fields
[[models]]
name = "meta-llama/llama-4-maverick-17b-128e-instruct"
display_name = "Llama 4 Maverick"  # appears in UI
provider = "Groq"  # appears in UI
temperature = 1.0  # high temp = high variation in output
max_retries = 0  # number of retries on failed request
api_base = "https://api.groq.com/openai/v1"
api_key = ""
